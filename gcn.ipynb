{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800]) torch.Size([32, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.randn(32, 1, 5, 5)\n",
    "# flatten to 1D\n",
    "m = nn.Flatten(start_dim=0, end_dim=-1)\n",
    "output = m(input)\n",
    "print(output.size(), input.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Planetoid\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCNConv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the Cora dataset\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7950\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "# get the shape of data\n",
    "print(data.x.size())\n",
    "_, pred = model(data).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "loss_fn = nn.NLLLoss()\n",
    "# input to NLLLoss is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target must have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "print(log_softmax(input).shape, target.shape)\n",
    "loss = loss_fn(log_softmax(input), target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.cuda.FloatTensor' as parameter 'correlation' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m image_batch \u001b[38;5;241m=\u001b[39m image_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m label_batch \u001b[38;5;241m=\u001b[39m label_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 52\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(output[0], label_batch[0])\u001b[39;00m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), label_batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/giv/sgrid-graph/graph.py:69\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, adj))\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# avg pooling\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/giv/sgrid-graph/graph.py:40\u001b[0m, in \u001b[0;36mGraphConvolution.forward\u001b[0;34m(self, input, adj)\u001b[0m\n\u001b[1;32m     38\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(adj, support)\n\u001b[1;32m     39\u001b[0m zero_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9e15\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(adj)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelation\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(adj \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrelation, zero_vec)\n\u001b[1;32m     41\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrelation, output)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1635\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1636\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1637\u001b[0m                         \u001b[38;5;241m.\u001b[39mformat(torch\u001b[38;5;241m.\u001b[39mtypename(value), name))\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.cuda.FloatTensor' as parameter 'correlation' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from graph import GCN\n",
    "from utils import image_to_adj\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the MNIST dataset\n",
    "image_size = 8\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), transforms.Resize(image_size, antialias=True)])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "adj = image_to_adj(np.zeros((image_size, image_size)))\n",
    "# adj is A + I\n",
    "adj = adj + torch.eye(adj.size(0))\n",
    "\n",
    "model = GCN(1, 16, 10, 0.5, image_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    adj = adj.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "n_epochs = 20\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (image_batch, label_batch) in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        image_batch = image_batch.float().squeeze().view(train_loader.batch_size, image_size * image_size, image_batch.size(1))\n",
    "        image_batch = image_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "        \n",
    "        output = model(image_batch, adj)\n",
    "        # print(output[0], label_batch[0])\n",
    "        loss = criterion(output.squeeze(), label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "\n",
    "    print(f'Epoch {epoch}, loss: {loss}')\n",
    "print('Done!')\n",
    "\n",
    "print('Test:')\n",
    "model.eval()\n",
    "\n",
    "test_data = tv.datasets.MNIST(root='./data', train=False, download=True)\n",
    "test_labels = test_data.targets.to(device)\n",
    "test_data = tv.transforms.Resize(8, antialias=True)(test_data.data).to(device)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "for i, (image, label) in enumerate(test_loader):\n",
    "    image = image.float().squeeze().view(test_loader.batch_size, image_size * image_size, image.size(1))\n",
    "    label = label.to(device)\n",
    "    image = image.to(device)\n",
    "    output = model(image, adj).squeeze()\n",
    "    correct += (output.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "print(f'Accuracy: {correct / len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:12, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.2878103256225586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:10, 26.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.267825126647949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:11, 26.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 2.26930570602417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:11, 26.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 2.2903623580932617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 2.2627410888671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 23.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 2.2820699214935303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:19, 23.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 2.259732484817505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 23.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 2.2606565952301025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:19, 23.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.247312068939209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:16, 24.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 2.249415397644043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:16, 24.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 2.2699949741363525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:14, 25.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 2.251877784729004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:17, 24.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, loss: 2.258760929107666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:19, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss: 2.2558648586273193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:19, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, loss: 2.2518107891082764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:19, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, loss: 2.2629952430725098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 23.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, loss: 2.2483417987823486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:20, 23.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, loss: 2.265115261077881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:21, 23.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, loss: 2.2534408569335938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 23.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, loss: 2.2535367012023926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 23.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, loss: 2.2596137523651123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:17, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 2.240478038787842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:19, 23.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, loss: 2.281371593475342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 23.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, loss: 2.246974468231201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:18, 23.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, loss: 2.2519590854644775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:14, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, loss: 2.231355667114258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:14, 25.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, loss: 2.2457385063171387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:14, 25.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, loss: 2.2700252532958984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:13, 25.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, loss: 2.273273468017578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [01:14, 25.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, loss: 2.2537271976470947\n",
      "Done!\n",
      "Test:\n",
      "Accuracy: 0.2825\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gat import GAT\n",
    "from utils import image_to_adj\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the MNIST dataset\n",
    "image_size = 16\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), transforms.Resize(image_size, antialias=True)])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "adj = image_to_adj(train_dataset[0][0].squeeze())\n",
    "# adj is A + I\n",
    "# adj = adj + torch.eye(adj.size(0))\n",
    "\n",
    "model = GAT(n_feat=1, n_class=10, n_layer=2, agg_hidden=32, fc_hidden=64, device=device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    adj = adj.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "n_epochs = 30\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (image, label_batch) in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        image = image.float().squeeze().view(image.size(0), image_size * image_size, image.size(1))\n",
    "        image = image.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "        \n",
    "        output = model(image, adj)\n",
    "        # print(output[0], label_batch[0])\n",
    "        loss = criterion(output.squeeze(), label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "\n",
    "    print(f'Epoch {epoch}, loss: {loss}')\n",
    "print('Done!')\n",
    "\n",
    "print('Test:')\n",
    "model.eval()\n",
    "\n",
    "test_data = tv.datasets.MNIST(root='./data', train=False, download=True)\n",
    "test_labels = test_data.targets.to(device)\n",
    "test_data = tv.transforms.Resize(8, antialias=True)(test_data.data).to(device)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "for i, (image, label) in enumerate(test_loader):\n",
    "    image = image.float().squeeze().view(image.size(0), image_size * image_size, image.size(1))\n",
    "    label = label.to(device)\n",
    "    image = image.to(device)\n",
    "    output = model(image, adj).squeeze()\n",
    "    correct += (output.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "print(f'Accuracy: {correct / len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7a8a09d62950>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjcElEQVR4nO3dfXBU9b3H8c9GyEbG7EIqySYQEESDPAbC08YOxBpNgWHMnc5cSp0GKWB1oAPFaSWdXrnivW69SrXTUh7GUXqrGSytwC1VaAwFRgkggYyAFMtDCWo2aIFdiO2K2d/9w3E1koQE92w2v7xfM2fGPfmds1/WnXlzNrusyxhjBACAxVI6ewAAAJxG7AAA1iN2AADrETsAgPWIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1nMsdufOndO9994rj8ej3r17a+7cubp06VKbxxQVFcnlcjXbHnjgAadGBAB0Ey6n/m3MqVOnqr6+XmvWrNHly5c1Z84cjR8/XhUVFa0eU1RUpFtvvVXLly+P7evVq5c8Ho8TIwIAuokeTpz06NGj2rp1q958802NGzdOkvTLX/5S06ZN01NPPaWcnJxWj+3Vq5d8Pp8TYwEAuilHYlddXa3evXvHQidJxcXFSklJ0d69e/Vv//ZvrR774osv6oUXXpDP59OMGTP0H//xH+rVq1er6yORiCKRSOx2NBrVuXPn9LWvfU0ulys+fyAAQMIYY3Tx4kXl5OQoJSU+v21zJHbBYFCZmZnN76hHD2VkZCgYDLZ63He+8x0NHDhQOTk5euutt/Twww/r2LFjevnll1s9JhAI6NFHH43b7ACA5HDmzBn1798/LufqUOyWLl2qJ554os01R48eveZh7r///th/jxw5UtnZ2brzzjt14sQJ3XzzzS0eU15eriVLlsRuh0IhDRgwQF/XNPVQz2ueBe238Z1DnT0CAIuEL0U1cOzflZ6eHrdzdih2Dz30kO6777421wwePFg+n09nz55ttv+TTz7RuXPnOvT7uIkTJ0qSjh8/3mrs3G633G73Fft7qKd6uIhdInjS+QQLgPiL56+iOhS7vn37qm/fvldd5/f7deHCBdXU1KigoECStH37dkWj0VjA2qO2tlaSlJ2d3ZExAQBoxpG/kt9222365je/qfnz52vfvn164403tHDhQn3729+OvRPzvffe09ChQ7Vv3z5J0okTJ/TYY4+ppqZGf//73/V///d/Kisr0+TJkzVq1CgnxgQAdBOOvf704osvaujQobrzzjs1bdo0ff3rX9fatWtjP798+bKOHTumjz76SJKUmpqq1157TXfffbeGDh2qhx56SN/61rf0xz/+0akRAQDdhGMfKu8s4XBYXq9XRbqH39klyLb3azt7BAAWCV+Mqs+tJxUKheL2j4rwzgIAgPWIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1iN2AADrETsAgPWIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1iN2AADrETsAgPWIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1iN2AADrETsAgPWIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1iN2AADrETsAgPWIHQDAesQOAGA9x2O3cuVK3XTTTUpLS9PEiRO1b9++Ntdv2LBBQ4cOVVpamkaOHKlXXnnF6REBAJZzNHYvvfSSlixZomXLlunAgQMaPXq0SkpKdPbs2RbX7969W7NmzdLcuXN18OBBlZaWqrS0VIcPH3ZyTACA5VzGGOPUySdOnKjx48frV7/6lSQpGo0qNzdXP/jBD7R06dIr1s+cOVONjY3asmVLbN+kSZOUn5+v1atXt+s+w+GwvF6vinSPerh6xucPgjZte7+2s0cAYJHwxaj63HpSoVBIHo8nLud07Mru448/Vk1NjYqLiz+/s5QUFRcXq7q6usVjqqurm62XpJKSklbXS1IkElE4HG62AQDwRY7F7sMPP1RTU5OysrKa7c/KylIwGGzxmGAw2KH1khQIBOT1emNbbm7uVx8eAGCVLv9uzPLycoVCodh25syZzh4JAJBkejh14htvvFHXXXedGhoamu1vaGiQz+dr8Rifz9eh9ZLkdrvldru/+sAAAGs5dmWXmpqqgoICVVVVxfZFo1FVVVXJ7/e3eIzf72+2XpIqKytbXQ8AQHs4dmUnSUuWLNHs2bM1btw4TZgwQc8884waGxs1Z84cSVJZWZn69eunQCAgSVq0aJGmTJmiFStWaPr06Vq/fr3279+vtWvXOjkmAMByjsZu5syZ+uCDD/TII48oGAwqPz9fW7dujb0Jpa6uTikpn19cFhYWqqKiQj/96U/1k5/8RLfccos2bdqkESNGODkmAMByjn7OrjPwObvE43N2AOKpS33ODgCAZEHsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOs5HruVK1fqpptuUlpamiZOnKh9+/a1unbdunVyuVzNtrS0NKdHBABYztHYvfTSS1qyZImWLVumAwcOaPTo0SopKdHZs2dbPcbj8ai+vj62nT592skRAQDdgKOx+/nPf6758+drzpw5GjZsmFavXq1evXrpueeea/UYl8sln88X27KyspwcEQDQDfRw6sQff/yxampqVF5eHtuXkpKi4uJiVVdXt3rcpUuXNHDgQEWjUY0dO1aPP/64hg8f3ur6SCSiSCQSux0OhyVJG985JE86v5JMhJKc/M4eodvZ9n5tZ48AdCmO1eDDDz9UU1PTFVdmWVlZCgaDLR6Tl5en5557Tps3b9YLL7ygaDSqwsJCvfvuu63eTyAQkNfrjW25ublx/XMAALq+pLr08fv9KisrU35+vqZMmaKXX35Zffv21Zo1a1o9pry8XKFQKLadOXMmgRMDALoCx17GvPHGG3XdddepoaGh2f6Ghgb5fL52naNnz54aM2aMjh8/3uoat9stt9v9lWYFANjNsSu71NRUFRQUqKqqKrYvGo2qqqpKfr+/XedoamrSoUOHlJ2d7dSYAIBuwLErO0lasmSJZs+erXHjxmnChAl65pln1NjYqDlz5kiSysrK1K9fPwUCAUnS8uXLNWnSJA0ZMkQXLlzQk08+qdOnT2vevHlOjgkAsJyjsZs5c6Y++OADPfLIIwoGg8rPz9fWrVtjb1qpq6tTSsrnF5fnz5/X/PnzFQwG1adPHxUUFGj37t0aNmyYk2MCACznMsaYzh4insLhsLxer86/M5iPHiQIHz1IPD56AJuFL0bV59aTCoVC8ng8cTknNQAAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALAesQMAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALAesQMAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALAesQMAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALAesQMAWI/YAQCsR+wAANYjdgAA6zkau127dmnGjBnKycmRy+XSpk2brnrMjh07NHbsWLndbg0ZMkTr1q1zckQAQDfgaOwaGxs1evRorVy5sl3rT506penTp+uOO+5QbW2tFi9erHnz5mnbtm1OjgkAsFwPJ08+depUTZ06td3rV69erUGDBmnFihWSpNtuu02vv/66nn76aZWUlLR4TCQSUSQSid0Oh8NfbWgAgHWS6nd21dXVKi4ubravpKRE1dXVrR4TCATk9XpjW25urtNjAgC6mKSKXTAYVFZWVrN9WVlZCofD+uc//9niMeXl5QqFQrHtzJkziRgVANCFOPoyZiK43W653e7OHgMAkMSS6srO5/OpoaGh2b6GhgZ5PB5df/31nTQVAKCrS6rY+f1+VVVVNdtXWVkpv9/fSRMBAGzgaOwuXbqk2tpa1dbWSvr0owW1tbWqq6uT9Onv28rKymLrH3jgAZ08eVI//vGP9de//lW//vWv9bvf/U4//OEPnRwTAGA5R2O3f/9+jRkzRmPGjJEkLVmyRGPGjNEjjzwiSaqvr4+FT5IGDRqkP/3pT6qsrNTo0aO1YsUKPfvss61+7AAAgPZwGWNMZw8RT+FwWF6vV+ffGSxPelK9Smutkpz8zh6h29n2fm1njwA4Jnwxqj63nlQoFJLH44nLOakBAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwnqOx27Vrl2bMmKGcnBy5XC5t2rSpzfU7duyQy+W6YgsGg06OCQCwnKOxa2xs1OjRo7Vy5coOHXfs2DHV19fHtszMTIcmBAB0Bz2cPPnUqVM1derUDh+XmZmp3r17t2ttJBJRJBKJ3Q6Hwx2+PwCA3RyN3bXKz89XJBLRiBEj9J//+Z+6/fbbW10bCAT06KOPJnA6fNm292s7e4RupyQnv7NH6FZ4jnd9SfUGlezsbK1evVp/+MMf9Ic//EG5ubkqKirSgQMHWj2mvLxcoVAotp05cyaBEwMAuoKkurLLy8tTXl5e7HZhYaFOnDihp59+Wr/97W9bPMbtdsvtdidqRABAF5RUV3YtmTBhgo4fP97ZYwAAurCkj11tba2ys7M7ewwAQBfm6MuYly5danZVdurUKdXW1iojI0MDBgxQeXm53nvvPf3v//6vJOmZZ57RoEGDNHz4cP3rX//Ss88+q+3bt+vPf/6zk2MCACznaOz279+vO+64I3Z7yZIlkqTZs2dr3bp1qq+vV11dXeznH3/8sR566CG999576tWrl0aNGqXXXnut2TkAAOgolzHGdPYQ8RQOh+X1enX+ncHypCf9q7TANeGjB4nFRw8SK3wxqj63nlQoFJLH44nLOakBAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwnqOxCwQCGj9+vNLT05WZmanS0lIdO3bsqsdt2LBBQ4cOVVpamkaOHKlXXnnFyTEBAJZzNHY7d+7UggULtGfPHlVWVury5cu6++671djY2Ooxu3fv1qxZszR37lwdPHhQpaWlKi0t1eHDh50cFQBgMZcxxiTqzj744ANlZmZq586dmjx5cotrZs6cqcbGRm3ZsiW2b9KkScrPz9fq1auveh/hcFher1fn3xksTzqv0sJOJTn5nT1Ct7Lt/drOHqFbCV+Mqs+tJxUKheTxeOJyzoTWIBQKSZIyMjJaXVNdXa3i4uJm+0pKSlRdXd3i+kgkonA43GwDAOCLEha7aDSqxYsX6/bbb9eIESNaXRcMBpWVldVsX1ZWloLBYIvrA4GAvF5vbMvNzY3r3ACAri9hsVuwYIEOHz6s9evXx/W85eXlCoVCse3MmTNxPT8AoOvrkYg7WbhwobZs2aJdu3apf//+ba71+XxqaGhotq+hoUE+n6/F9W63W263O26zAgDs4+iVnTFGCxcu1MaNG7V9+3YNGjToqsf4/X5VVVU121dZWSm/3+/UmAAAyzl6ZbdgwQJVVFRo8+bNSk9Pj/3ezev16vrrr5cklZWVqV+/fgoEApKkRYsWacqUKVqxYoWmT5+u9evXa//+/Vq7dq2TowIALObold2qVasUCoVUVFSk7Ozs2PbSSy/F1tTV1am+vj52u7CwUBUVFVq7dq1Gjx6t3//+99q0aVObb2oBAKAtCf2cXSLwOTt0B3zOLrH4nF1idfnP2QEA0BmIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1iN2AADrETsAgPWIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1iN2AADrETsAgPWIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1iN2AADrETsAgPWIHQDAesQOAGA9YgcAsB6xAwBYj9gBAKxH7AAA1iN2AADrETsAgPWIHQDAesQOAGA9R2MXCAQ0fvx4paenKzMzU6WlpTp27Fibx6xbt04ul6vZlpaW5uSYAADLORq7nTt3asGCBdqzZ48qKyt1+fJl3X333WpsbGzzOI/Ho/r6+th2+vRpJ8cEAFiuh5Mn37p1a7Pb69atU2ZmpmpqajR58uRWj3O5XPL5fE6OBgDoRhyN3ZeFQiFJUkZGRpvrLl26pIEDByoajWrs2LF6/PHHNXz48BbXRiIRRSKR2O1wOBy/gYEkte392s4eoVspycnv7BG6lU/MZUkn43rOhL1BJRqNavHixbr99ts1YsSIVtfl5eXpueee0+bNm/XCCy8oGo2qsLBQ7777bovrA4GAvF5vbMvNzXXqjwAA6KJcxhiTiDt68MEH9eqrr+r1119X//79233c5cuXddttt2nWrFl67LHHrvh5S1d2ubm5Ov/OYHnSebMpgK+OK7vE+sRc1g5tVigUksfjics5E/Iy5sKFC7Vlyxbt2rWrQ6GTpJ49e2rMmDE6fvx4iz93u91yu93xGBMAYClHL32MMVq4cKE2btyo7du3a9CgQR0+R1NTkw4dOqTs7GwHJgQAdAeOXtktWLBAFRUV2rx5s9LT0xUMBiVJXq9X119/vSSprKxM/fr1UyAQkCQtX75ckyZN0pAhQ3ThwgU9+eSTOn36tObNm+fkqAAAizkau1WrVkmSioqKmu1//vnndd9990mS6urqlJLy+QXm+fPnNX/+fAWDQfXp00cFBQXavXu3hg0b5uSoAACLJewNKokSDofl9Xp5gwqAuOENKonlxBtUqAEAwHrEDgBgPWIHALAesQMAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALAesQMAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALAesQMAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALAesQMAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALCeo7FbtWqVRo0aJY/HI4/HI7/fr1dffbXNYzZs2KChQ4cqLS1NI0eO1CuvvOLkiACAbsDR2PXv318/+9nPVFNTo/379+sb3/iG7rnnHh05cqTF9bt379asWbM0d+5cHTx4UKWlpSotLdXhw4edHBMAYDmXMcYk8g4zMjL05JNPau7cuVf8bObMmWpsbNSWLVti+yZNmqT8/HytXr26XecPh8Pyer06/85gedJ5lRbAV1eSk9/ZI3Qrn5jL2qHNCoVC8ng8cTlnwmrQ1NSk9evXq7GxUX6/v8U11dXVKi4ubravpKRE1dXVrZ43EokoHA432wAA+CLHY3fo0CHdcMMNcrvdeuCBB7Rx40YNGzasxbXBYFBZWVnN9mVlZSkYDLZ6/kAgIK/XG9tyc3PjOj8AoOtzPHZ5eXmqra3V3r179eCDD2r27Nl6++2343b+8vJyhUKh2HbmzJm4nRsAYIceTt9BamqqhgwZIkkqKCjQm2++qV/84hdas2bNFWt9Pp8aGhqa7WtoaJDP52v1/G63W263O75DAwCskvB3cESjUUUikRZ/5vf7VVVV1WxfZWVlq7/jAwCgPRy9sisvL9fUqVM1YMAAXbx4URUVFdqxY4e2bdsmSSorK1O/fv0UCAQkSYsWLdKUKVO0YsUKTZ8+XevXr9f+/fu1du1aJ8cEAFjO0didPXtWZWVlqq+vl9fr1ahRo7Rt2zbdddddkqS6ujqlpHx+cVlYWKiKigr99Kc/1U9+8hPdcsst2rRpk0aMGOHkmAAAyyX8c3ZO43N2AOKNz9klVpf+nB0AAJ2F2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOsROwCA9YgdAMB6xA4AYD1iBwCwHrEDAFiP2AEArEfsAADWI3YAAOs5GrtVq1Zp1KhR8ng88ng88vv9evXVV1tdv27dOrlcrmZbWlqakyMCALqBHk6evH///vrZz36mW265RcYY/eY3v9E999yjgwcPavjw4S0e4/F4dOzYsdhtl8vl5IgAgG7A0djNmDGj2e3//u//1qpVq7Rnz55WY+dyueTz+dp9H5FIRJFIJHY7FApJksKXotcwMQBc6RNzubNH6FY+0aePtzEmbud0NHZf1NTUpA0bNqixsVF+v7/VdZcuXdLAgQMVjUY1duxYPf74462GUZICgYAeffTRK/YPHPv3eIwNAJJOdvYA3dI//vEPeb3euJzLZeKZzhYcOnRIfr9f//rXv3TDDTeooqJC06ZNa3FtdXW1/va3v2nUqFEKhUJ66qmntGvXLh05ckT9+/dv8ZgvX9lduHBBAwcOVF1dXdwepEQIh8PKzc3VmTNn5PF4OnucDumqszN3YjF34nXV2UOhkAYMGKDz58+rd+/ecTmn41d2eXl5qq2tVSgU0u9//3vNnj1bO3fu1LBhw65Y6/f7m131FRYW6rbbbtOaNWv02GOPtXh+t9stt9t9xX6v19ul/ud+5rM383RFXXV25k4s5k68rjp7Skr83kPpeOxSU1M1ZMgQSVJBQYHefPNN/eIXv9CaNWuuemzPnj01ZswYHT9+3OkxAQAWS/jn7KLRaLOXHdvS1NSkQ4cOKTs72+GpAAA2c/TKrry8XFOnTtWAAQN08eJFVVRUaMeOHdq2bZskqaysTP369VMgEJAkLV++XJMmTdKQIUN04cIFPfnkkzp9+rTmzZvX7vt0u91atmxZiy9tJrOuOrfUdWdn7sRi7sTrqrM7Mbejb1CZO3euqqqqVF9fL6/Xq1GjRunhhx/WXXfdJUkqKirSTTfdpHXr1kmSfvjDH+rll19WMBhUnz59VFBQoP/6r//SmDFjnBoRANANOP5uTAAAOhv/NiYAwHrEDgBgPWIHALAesQMAWM+K2J07d0733nuvPB6Pevfurblz5+rSpUttHlNUVHTF1wk98MADjs65cuVK3XTTTUpLS9PEiRO1b9++Ntdv2LBBQ4cOVVpamkaOHKlXXnnF0fna0pHZk+Grmnbt2qUZM2YoJydHLpdLmzZtuuoxO3bs0NixY+V2uzVkyJDYu4QTraOz79ix44rH2+VyKRgMJmZgffpv1I4fP17p6enKzMxUaWlps28vaU1nP8evZe5keH5LHf8KNanzH2+p8776zYrY3XvvvTpy5IgqKyu1ZcsW7dq1S/fff/9Vj5s/f77q6+tj2//8z/84NuNLL72kJUuWaNmyZTpw4IBGjx6tkpISnT17tsX1u3fv1qxZszR37lwdPHhQpaWlKi0t1eHDhx2bsTUdnV369J8n+uJje/r06QROLDU2Nmr06NFauXJlu9afOnVK06dP1x133KHa2lotXrxY8+bNi30mNJE6Ovtnjh071uwxz8zMdGjCK+3cuVMLFizQnj17VFlZqcuXL+vuu+9WY2Njq8ckw3P8WuaWOv/5LX3+FWo1NTXav3+/vvGNb+iee+7RkSNHWlyfDI/3tcwtxenxNl3c22+/bSSZN998M7bv1VdfNS6Xy7z33nutHjdlyhSzaNGiBEz4qQkTJpgFCxbEbjc1NZmcnBwTCARaXP/v//7vZvr06c32TZw40Xz/+993dM6WdHT2559/3ni93gRNd3WSzMaNG9tc8+Mf/9gMHz682b6ZM2eakpISBye7uvbM/pe//MVIMufPn0/ITO1x9uxZI8ns3Lmz1TXJ9Bz/THvmTrbn9xf16dPHPPvssy3+LBkf78+0NXe8Hu8uf2VXXV2t3r17a9y4cbF9xcXFSklJ0d69e9s89sUXX9SNN96oESNGqLy8XB999JEjM3788ceqqalRcXFxbF9KSoqKi4tVXV3d4jHV1dXN1ktSSUlJq+udci2zS59/VVNubu5V/9aWDJLl8f4q8vPzlZ2drbvuuktvvPFGp87y2fdKZmRktLomGR/z9swtJd/zu6mpSevXr2/zK9SS8fFuz9xSfB7vhH2fnVOCweAVL9f06NFDGRkZbf7O4jvf+Y4GDhyonJwcvfXWW3r44Yd17Ngxvfzyy3Gf8cMPP1RTU5OysrKa7c/KytJf//rXFo8JBoMtrk/k72Gka5s9Ly9Pzz33XLOvaiosLGzzq5o6W2uPdzgc1j//+U9df/31nTTZ1WVnZ2v16tUaN26cIpGInn32WRUVFWnv3r0aO3ZswueJRqNavHixbr/9do0YMaLVdcnyHP9Me+dOpuf3l79CbePGjS1+o4yUXI93R+aO1+OdtLFbunSpnnjiiTbXHD169JrP/8Xf6Y0cOVLZ2dm68847deLECd18883XfF5c21c14drl5eUpLy8vdruwsFAnTpzQ008/rd/+9rcJn2fBggU6fPiwXn/99YTf91fR3rmT6fndka9QSyZOf/VbS5I2dg899JDuu+++NtcMHjxYPp/vijdKfPLJJzp37px8Pl+772/ixImSpOPHj8c9djfeeKOuu+46NTQ0NNvf0NDQ6ow+n69D651yLbN/WVf4qqbWHm+Px5PUV3WtmTBhQqfEZuHChbE3iV3tb93J8hyXOjb3l3Xm87sjX6GWTI93Z3z1W9L+zq5v374aOnRom1tqaqr8fr8uXLigmpqa2LHbt29XNBqNBaw9amtrJcmRrxNKTU1VQUGBqqqqYvui0aiqqqpafZ3a7/c3Wy9JlZWVbb6u7YRrmf3LusJXNSXL4x0vtbW1CX28jTFauHChNm7cqO3bt2vQoEFXPSYZHvNrmfvLkun53dZXqCXD492ahHz121d+i0sS+OY3v2nGjBlj9u7da15//XVzyy23mFmzZsV+/u6775q8vDyzd+9eY4wxx48fN8uXLzf79+83p06dMps3bzaDBw82kydPdmzG9evXG7fbbdatW2fefvttc//995vevXubYDBojDHmu9/9rlm6dGls/RtvvGF69OhhnnrqKXP06FGzbNky07NnT3Po0CHHZozX7I8++qjZtm2bOXHihKmpqTHf/va3TVpamjly5EjCZr548aI5ePCgOXjwoJFkfv7zn5uDBw+a06dPG2OMWbp0qfnud78bW3/y5EnTq1cv86Mf/cgcPXrUrFy50lx33XVm69atCZv5Wmd/+umnzaZNm8zf/vY3c+jQIbNo0SKTkpJiXnvttYTN/OCDDxqv12t27Nhh6uvrY9tHH30UW5OMz/FrmTsZnt/GfPo82Llzpzl16pR56623zNKlS43L5TJ//vOfW5w7GR7va5k7Xo+3FbH7xz/+YWbNmmVuuOEG4/F4zJw5c8zFixdjPz916pSRZP7yl78YY4ypq6szkydPNhkZGcbtdpshQ4aYH/3oRyYUCjk65y9/+UszYMAAk5qaaiZMmGD27NkT+9mUKVPM7Nmzm63/3e9+Z2699VaTmppqhg8fbv70pz85Ol9bOjL74sWLY2uzsrLMtGnTzIEDBxI672dvx//y9tmcs2fPNlOmTLnimPz8fJOammoGDx5snn/++YTO/MU5OjL7E088YW6++WaTlpZmMjIyTFFRkdm+fXtCZ25pXknNHsNkfI5fy9zJ8Pw2xpjvfe97ZuDAgSY1NdX07dvX3HnnnbFgtDS3MZ3/eBvT8bnj9XjzFT8AAOsl7e/sAACIF2IHALAesQMAWI/YAQCsR+wAANYjdgAA6xE7AID1iB0AwHrEDgBgPWIHALAesQMAWO//Aakn+VWqx2lxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import image_to_adj\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "temp = np.zeros((2, 2))\n",
    "adj = image_to_adj(temp)\n",
    "\n",
    "adj = adj.cpu().detach().numpy()\n",
    "adj = adj - np.eye(adj.shape[0])\n",
    "plt.imshow(adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "a = torch.from_numpy(adj)\n",
    "torch.where(a > 0, a, torch.tensor(1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
